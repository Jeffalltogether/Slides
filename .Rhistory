learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
#plot of actual accuracy and predicted accuracy
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
###Course Project Development
# Goal -train linear ML algorithm and report accuracy (kappa) and predict the
# learning curve for the predicsion on the set of data where user inputs the
library(caret)
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
### separate data into training and test data
set.seed(1177)
# trainIndex <- createDataPartition(iris$Species, p = 0.70, list=FALSE)
# training <- iris[trainIndex,]
# testing <- iris[-trainIndex,]
training <- iris
### Train lda Classifier
#fit model
# slider... n should be from 2 - 90
n <- 13
z <- 9
training_cases <- rep(0, n-z)
in_sample <- rep(0, n-z)
out_sample <- rep(0, n-z)
for (i in z:n){
classA <- grep("setosa", training$Species)
classB <- grep("versicolor", training$Species)
classC <- grep("virginica", training$Species)
x <- floor(i/3)+ceiling(i/3)+ceiling(i/3)
y <- floor(i/3)+floor(i/3)+ceiling(i/3)
#Separate the input number of training data cases into three integers
if (x == i){
numRowsToSelect = c(floor(i/3),ceiling(i/3),ceiling(i/3))
}
if (y == i){
numRowsToSelect = c(floor(i/3),floor(i/3),ceiling(i/3))
}
Acases = numRowsToSelect[1]
Bcases = numRowsToSelect[2]
Ccases = numRowsToSelect[3]
#determine if the A class has adequate training cases to suppor the desired number
if (numRowsToSelect[1] > length(classA)){
Acases = length(classA)
}
#determine if the B class has adequate training cases to suppor the desired number
if (numRowsToSelect[2] > length(classB)){
Bcases = length(classB)
}
#determine if the C class has adequate training cases to suppor the desired number
if (numRowsToSelect[3] > length(classC)){
Ccases = length(classB)
}
rm(includeRows)
includeRows <- c(classA[1:Acases], classB[1:Bcases], classC[1:Ccases])
Lda <- train(Species ~ ., method="lda", data=training[includeRows,])
# pred <- predict(Lda, testing)
# result <- confusionMatrix(testing$Species, pred)
training_cases[i-z] <- length(includeRows)
in_sample[i-z] <- as.numeric(Lda$results$Kappa)
# out_sample[i-z] <- as.numeric(result$overall[1])
}
rm(x,y)
# learningData <- data.frame(training_cases, in_sample, out_sample)
# colnames(learningData) <- c("training_cases", "in_sample", "out_sample")
learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
#plot of actual accuracy and predicted accuracy
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
scale_y_continuous(limits = c(0,110))
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
scale_y_continuous(limits = c(0,150))
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
#plot of actual accuracy and predicted accuracy
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
###Course Project Development
# Goal -train linear ML algorithm and report accuracy (kappa) and predict the
# learning curve for the predicsion on the set of data where user inputs the
library(caret)
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
### separate data into training and test data
set.seed(1177)
# trainIndex <- createDataPartition(iris$Species, p = 0.70, list=FALSE)
# training <- iris[trainIndex,]
# testing <- iris[-trainIndex,]
training <- iris
### Train lda Classifier
#fit model
# slider... n should be from 2 - 90
n <- 14
z <- 9
training_cases <- rep(0, n-z)
in_sample <- rep(0, n-z)
out_sample <- rep(0, n-z)
for (i in z:n){
classA <- grep("setosa", training$Species)
classB <- grep("versicolor", training$Species)
classC <- grep("virginica", training$Species)
x <- floor(i/3)+ceiling(i/3)+ceiling(i/3)
y <- floor(i/3)+floor(i/3)+ceiling(i/3)
#Separate the input number of training data cases into three integers
if (x == i){
numRowsToSelect = c(floor(i/3),ceiling(i/3),ceiling(i/3))
}
if (y == i){
numRowsToSelect = c(floor(i/3),floor(i/3),ceiling(i/3))
}
Acases = numRowsToSelect[1]
Bcases = numRowsToSelect[2]
Ccases = numRowsToSelect[3]
#determine if the A class has adequate training cases to suppor the desired number
if (numRowsToSelect[1] > length(classA)){
Acases = length(classA)
}
#determine if the B class has adequate training cases to suppor the desired number
if (numRowsToSelect[2] > length(classB)){
Bcases = length(classB)
}
#determine if the C class has adequate training cases to suppor the desired number
if (numRowsToSelect[3] > length(classC)){
Ccases = length(classB)
}
rm(includeRows)
includeRows <- c(classA[1:Acases], classB[1:Bcases], classC[1:Ccases])
Lda <- train(Species ~ ., method="lda", data=training[includeRows,])
# pred <- predict(Lda, testing)
# result <- confusionMatrix(testing$Species, pred)
training_cases[i-z] <- length(includeRows)
in_sample[i-z] <- as.numeric(Lda$results$Kappa)
# out_sample[i-z] <- as.numeric(result$overall[1])
}
rm(x,y)
# learningData <- data.frame(training_cases, in_sample, out_sample)
# colnames(learningData) <- c("training_cases", "in_sample", "out_sample")
learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
#plot of actual accuracy and predicted accuracy
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
shiny::runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
?renderPlot
cases=50
learningData[cases,]
###Course Project Development
# Goal -train linear ML algorithm and report accuracy (kappa) and predict the
# learning curve for the predicsion on the set of data where user inputs the
library(caret)
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
### separate data into training and test data
set.seed(1177)
# trainIndex <- createDataPartition(iris$Species, p = 0.70, list=FALSE)
# training <- iris[trainIndex,]
# testing <- iris[-trainIndex,]
training <- iris
### Train lda Classifier
#fit model
# slider... n should be from 2 - 90
n <- 75
z <- 9
training_cases <- rep(0, n-z)
in_sample <- rep(0, n-z)
out_sample <- rep(0, n-z)
for (i in z:n){
classA <- grep("setosa", training$Species)
classB <- grep("versicolor", training$Species)
classC <- grep("virginica", training$Species)
x <- floor(i/3)+ceiling(i/3)+ceiling(i/3)
y <- floor(i/3)+floor(i/3)+ceiling(i/3)
#Separate the input number of training data cases into three integers
if (x == i){
numRowsToSelect = c(floor(i/3),ceiling(i/3),ceiling(i/3))
}
if (y == i){
numRowsToSelect = c(floor(i/3),floor(i/3),ceiling(i/3))
}
Acases = numRowsToSelect[1]
Bcases = numRowsToSelect[2]
Ccases = numRowsToSelect[3]
#determine if the A class has adequate training cases to suppor the desired number
if (numRowsToSelect[1] > length(classA)){
Acases = length(classA)
}
#determine if the B class has adequate training cases to suppor the desired number
if (numRowsToSelect[2] > length(classB)){
Bcases = length(classB)
}
#determine if the C class has adequate training cases to suppor the desired number
if (numRowsToSelect[3] > length(classC)){
Ccases = length(classB)
}
rm(includeRows)
includeRows <- c(classA[1:Acases], classB[1:Bcases], classC[1:Ccases])
Lda <- train(Species ~ ., method="lda", data=training[includeRows,])
# pred <- predict(Lda, testing)
# result <- confusionMatrix(testing$Species, pred)
training_cases[i-z] <- length(includeRows)
in_sample[i-z] <- as.numeric(Lda$results$Kappa)
# out_sample[i-z] <- as.numeric(result$overall[1])
}
rm(x,y)
# learningData <- data.frame(training_cases, in_sample, out_sample)
# colnames(learningData) <- c("training_cases", "in_sample", "out_sample")
learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
#plot of actual accuracy and predicted accuracy
p1 <- ggplot(learningData, aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[15,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
learningData[15,]
p1 <- ggplot(learningData[1:15,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[1:10,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[1:5,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[1:2,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[1:3,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
p1 <- ggplot(learningData[1:4,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 2, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50, 80), limits = c(9,80)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\nDetermined by 25-Fold Bootstrap Samples")
p1
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
shiny::runApp('09_DataProducts/DataProductsRepo/CourseProject/ShinyApp')
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo/CourseProject/ShinyApp")
library(shinyapps)
deployApp(appName = "Iris_Discriminant_Analysis_learning_Curve")
?xtable
library(caret)
library(ggplot2)
require(e1071)
library(xtable)
###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
xtable(head(iris), caption = "A subset of the Iris data set")
```{r echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
library(caret)
library(ggplot2)
require(e1071)
library(xtable)
###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
table1 <- xtable(iris[,c(10, 20, 75, 100, 125, 150)], caption = "A subset of the Iris data set")
print(table1, type="html")
library(caret)
library(ggplot2)
require(e1071)
library(xtable)
###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
table1 <- xtable(iris[,c(10, 20, 75, 100, 125, 150)], caption = "A subset of the Iris data set")
print(table1, type="html")
table1 <- xtable(iris[,c(10, 20, 75, 100, 125, 150)], caption = "A subset of the Iris data set")
iris[,c(10, 20, 75, 100, 125, 150)]
table1 <- xtable(iris[c(10, 20, 75, 100, 125, 150),], caption = "A subset of the Iris data set")
print(table1, type="html")
manipulate(plot(1:x), x=slider(1,100))
library(manipulate)
manipulate(plot(1:x), x=slider(1,100))
library(manipulate)
###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)
### separate data into training and test data
set.seed(1177)
training <- iris
### Train lda Classifier
#fit model
# slider... n should be from 9 - 45
n <- 45
z <- 9
training_cases <- rep(0, n-z)
in_sample <- rep(0, n-z)
out_sample <- rep(0, n-z)
allRows <- vector("list", n)
for (i in z:n){
classA <- grep("setosa", training$Species)
classB <- grep("versicolor", training$Species)
classC <- grep("virginica", training$Species)
x <- floor(i/3)+ceiling(i/3)+ceiling(i/3)
y <- floor(i/3)+floor(i/3)+ceiling(i/3)
#Separate the input number of training data cases into three integers
if (x == i){
numRowsToSelect = c(floor(i/3),ceiling(i/3),ceiling(i/3))
}
if (y == i){
numRowsToSelect = c(floor(i/3),floor(i/3),ceiling(i/3))
}
Acases = numRowsToSelect[1]
Bcases = numRowsToSelect[2]
Ccases = numRowsToSelect[3]
#determine if the A class has adequate training cases to suppor the desired number
if (numRowsToSelect[1] > length(classA)){
Acases = length(classA)
}
#determine if the B class has adequate training cases to suppor the desired number
if (numRowsToSelect[2] > length(classB)){
Bcases = length(classB)
}
#determine if the C class has adequate training cases to suppor the desired number
if (numRowsToSelect[3] > length(classC)){
Ccases = length(classB)
}
rm(includeRows)
includeRows <- c(classA[1:Acases], classB[1:Bcases], classC[1:Ccases])
#store all the rows used for each training subset
allRows[[i]] <- list(includeRows)
#Train Model - LDA
Lda <- train(Species ~ ., method="lda", data=training[includeRows,])
# pred <- predict(Lda, testing)
# result <- confusionMatrix(testing$Species, pred)
training_cases[i-z] <- length(includeRows)
in_sample[i-z] <- as.numeric(Lda$results$Kappa)
# out_sample[i-z] <- as.numeric(result$overall[1])
}
rm(x,y)
# learningData <- data.frame(training_cases, in_sample, out_sample)
# colnames(learningData) <- c("training_cases", "in_sample", "out_sample")
learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
manipulate(ggplot(iris[allRows[[cases+9]][[1]],],
aes(Sepal.Length, Petal.Length, color = Species)) +
geom_point(size = 4, alpha = 0.7) +
scale_x_continuous(limits = c(4,8)) +
scale_y_continuous(limits = c(0,8)) +
xlab("Sepal Length") +
ylab("Petal Length\n") +
theme(text = element_text(size=20), axis.text.x = element_text(hjust=1, size=18),
axis.text.y = element_text(hjust=1, size=18)),
cases = slider(12, 45, step = 1))
iris[allRows[[cases+9]][[1]],]
manipulate(ggplot(iris[allRows[[cases]][[1]],],
aes(Sepal.Length, Petal.Length, color = Species)) +
geom_point(size = 4, alpha = 0.7) +
scale_x_continuous(limits = c(4,8)) +
scale_y_continuous(limits = c(0,8)) +
xlab("Sepal Length") +
ylab("Petal Length\n") +
theme(text = element_text(size=20), axis.text.x = element_text(hjust=1, size=18),
axis.text.y = element_text(hjust=1, size=18)),
cases = slider(12, 45, step = 1))
cases = 45
ggplot(learningData[1:cases,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\n From 25 Bootstraped Data sets") +
theme(text = element_text(size=20), axis.text.x = element_text(hjust=1, size=18),
axis.text.y = element_text(hjust=1, size=18))
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/09_DataProducts/DataProductsRepo/CourseProject/Course_Project")
allRows[[cases]][[1]]
cases = 6
allRows[[cases]][[1]]
allRows[[cases]][[1]]
allRows[[cases]][[1]]
cases = 6
allRows[[cases]][[1]]
cases = 7
allRows[[cases]][[1]]
cases = 8
allRows[[cases]][[1]]
cases = 9
allRows[[cases]][[1]]
learningData[1:cases,]
cases = 12
p1 <- ggplot(iris[allRows[[cases+1]][[1]],],
aes(Sepal.Length, Petal.Length, color = Species)) +
geom_point(size = 4, alpha = 0.7) +
scale_x_continuous(limits = c(4,8)) +
scale_y_continuous(limits = c(0,8)) +
xlab("Sepal Length") +
ylab("Petal Length\n") +
theme(text = element_text(size=20), axis.text.x = element_text(hjust=1, size=18),
axis.text.y = element_text(hjust=1, size=18))
p2 <- ggplot(learningData[1:cases,], aes(training_cases, I(100*in_sample))) +
geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
geom_line(color = "forestgreen") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
geom_hline(yintercept = 100) +
xlab("Number of Training Data Samples") +
ylab("In-sample Accuracy [%]\n From 25 Bootstraped Sets") +
theme(text = element_text(size=16), axis.text.x = element_text(hjust=1, size=14),
axis.text.y = element_text(hjust=1, size=14))
allRows[[cases+1]][[1]]
?ggtitl
?ggtitle
