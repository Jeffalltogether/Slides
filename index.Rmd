---
title       : A Simple Exploration of the Machine Learning "Learning Curve"
subtitle    : Developing Data Products Course Project
author      : Jeff Thatcher
job         : Student
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
## Slide 1 - Why Inderstand the Learning Curve?
Data is expensive and time consuming to obtain. Therefore, estimations of training data size are necessary to predict the time and cost for building machine learning products.

* Learning curves are used in machine learning to predict the number of training cases need to obtain the highest achievable prediction accuracy.

* This presentation demonstrats how learning curves are one technique for predicting this size of a training data set.

* Consider using this technique during product development for applications and devices from which little data is available.

---
## Slide 2 - Linear Discriminant Analysis Learning Curve Example
In this presentation, we explore the Learning Curve concept using the Iris data set.

* The model we trained was a linear discriminant analysis (LDA) to predict the Iris species based on four features measured from each species.

Table. Subset of the Iris data set showing the four features and the three classes of species that the LDA algorithm will predict.

```{r echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
library(caret)
library(ggplot2)
require(e1071)
library(xtable)
library(manipulate)
source("multiplot.R")

###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)

table1 <- xtable(iris[c(1, 20, 75, 100, 125, 150),])
print(table1, type="html")
```




---
## Slide 3 - The Data Used
This figure displays a subset of the Iris data set used for training the machine learning model.

```{r echo=FALSE, warning=FALSE, message=FALSE}
###Load Data Set
data(iris)
iris <- iris
iris$Species <- as.factor(iris$Species)

### separate data into training and test data
set.seed(1177)
training <- iris

### Train lda Classifier
#fit model

# slider... n should be from 9 - 45
n <- 45
z <- 9
training_cases <- rep(0, n-z)
in_sample <- rep(0, n-z)
out_sample <- rep(0, n-z)
allRows <- vector("list", n)

for (i in z:n){
        classA <- grep("setosa", training$Species)
        classB <- grep("versicolor", training$Species)
        classC <- grep("virginica", training$Species)
        
        x <- floor(i/3)+ceiling(i/3)+ceiling(i/3)
        y <- floor(i/3)+floor(i/3)+ceiling(i/3)
        
        #Separate the input number of training data cases into three integers
        if (x == i){
                numRowsToSelect = c(floor(i/3),ceiling(i/3),ceiling(i/3))
        }
        
        if (y == i){
                numRowsToSelect = c(floor(i/3),floor(i/3),ceiling(i/3))
        }
        
        Acases = numRowsToSelect[1]
        Bcases = numRowsToSelect[2]
        Ccases = numRowsToSelect[3]
        
        #determine if the A class has adequate training cases to suppor the desired number
        if (numRowsToSelect[1] > length(classA)){
                Acases = length(classA)
        }
        
        #determine if the B class has adequate training cases to suppor the desired number
        if (numRowsToSelect[2] > length(classB)){
                Bcases = length(classB)
        }
        
        #determine if the C class has adequate training cases to suppor the desired number
        if (numRowsToSelect[3] > length(classC)){
                Ccases = length(classB)
        }
        
        rm(includeRows)
        includeRows <- c(classA[1:Acases], classB[1:Bcases], classC[1:Ccases])
        
        #store all the rows used for each training subset
        allRows[[i]] <- list(includeRows)
        
        #Train Model - LDA
        Lda <- train(Species ~ ., method="lda", data=training[includeRows,])
        
        # pred <- predict(Lda, testing)
        # result <- confusionMatrix(testing$Species, pred)
        
        
        training_cases[i-z] <- length(includeRows)
        in_sample[i-z] <- as.numeric(Lda$results$Kappa)
        # out_sample[i-z] <- as.numeric(result$overall[1])
}

rm(x,y)

# learningData <- data.frame(training_cases, in_sample, out_sample)
# colnames(learningData) <- c("training_cases", "in_sample", "out_sample")     

learningData <- data.frame(training_cases, in_sample)
colnames(learningData) <- c("training_cases", "in_sample")
```

```{r echo=FALSE, warning=FALSE, fig.height=4, fig.width=10}
cases = 45
# manipulate(
        ggplot(iris[allRows[[cases]][[1]],], 
        aes(Sepal.Length, Petal.Length, color = Species)) + 
        geom_point(size = 4, alpha = 0.7) +
        scale_x_continuous(limits = c(4,8)) +
        scale_y_continuous(limits = c(0,8)) +
        xlab("Sepal Length") +
        ylab("Petal Length\n") +
        theme(text = element_text(size=20), axis.text.x = element_text(hjust=1, size=18),
        axis.text.y = element_text(hjust=1, size=18)) #,
        #        cases = slider(12, 45, step = 1))
```

---
## Slide 4 - Learning Curve Characteristics

This slide shows the Learning Curve.

* x-axis - is the number of Irises included in the training data (i.e., number of rows)

* y-axis - is the accuracy of the LDA algorithm corresponding to the number of Irises used for training.

* Blue curve - is a smoothing function used to predict the future accuracy of the model when given more training data.

```{r echo=FALSE, warning=FALSE, fig.height=4, fig.width=10}
cases = 15
ggplot(learningData[1:cases,], aes(training_cases, I(100*in_sample))) + 
        geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
        geom_line(color = "forestgreen") +
        scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
        geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
        geom_hline(yintercept = 100) +
        xlab("Number of Training Data Samples") +
        ylab("In-sample Accuracy [%]\n From 25 Bootstraped Sets") +
        theme(text = element_text(size=16), axis.text.x = element_text(hjust=1, size=14),
        axis.text.y = element_text(hjust=1, size=14))
```


--- 
## Slide 5 - How the Learning Curve is Applied
As we change the size of the training data, we see an increase in classifier accuracy. Research shows that this trend holds true in data sets where classification algorithms work well.

* Here we present the results from  12, 15, and 20 data points used to train our LDA classifier

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=13}
cases = 12
p1 <- ggplot(iris[allRows[[cases]][[1]],], 
        aes(Sepal.Length, Petal.Length, color = Species)) + 
        geom_point(size = 4, alpha = 0.7) +
        scale_x_continuous(limits = c(4,8)) +
        scale_y_continuous(limits = c(0,8)) +
        xlab("Sepal Length") +
        ylab("Petal Length") +
        ggtitle("12 Data Points Included") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))

p2 <- ggplot(learningData[1:(cases-9),], aes(training_cases, I(100*in_sample))) + 
        geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
        geom_line(color = "forestgreen") +
        scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
        geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
        geom_hline(yintercept = 100) +
        xlab("Number of Training Data Samples") +
        ylab("In-sample Accuracy [%]") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))
######################
cases = 15
p3 <- ggplot(iris[allRows[[cases]][[1]],], 
        aes(Sepal.Length, Petal.Length, color = Species)) + 
        geom_point(size = 4, alpha = 0.7) +
        scale_x_continuous(limits = c(4,8)) +
        scale_y_continuous(limits = c(0,8)) +
        xlab("Sepal Length") +
        ylab("Petal Length") +
        ggtitle("15 Data Points Included") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))

p4 <- ggplot(learningData[1:(cases-9),], aes(training_cases, I(100*in_sample))) + 
        geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
        geom_line(color = "forestgreen") +
        scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
        geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
        geom_hline(yintercept = 100) +
        xlab("Number of Training Data Samples") +
        ylab("In-sample Accuracy [%]") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))
######################
cases = 20
p5 <- ggplot(iris[allRows[[cases]][[1]],], 
        aes(Sepal.Length, Petal.Length, color = Species)) + 
        geom_point(size = 4, alpha = 0.7) +
        scale_x_continuous(limits = c(4,8)) +
        scale_y_continuous(limits = c(0,8)) +
        xlab("Sepal Length") +
        ylab("Petal Length") +
        ggtitle("20 Data Points Included") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))

p6 <- ggplot(learningData[1:(cases-9),], aes(training_cases, I(100*in_sample))) + 
        geom_point(size = 4, alpha = 0.5, color="forestgreen", fill ="forestgreen") +
        geom_line(color = "forestgreen") +
        scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 50), limits = c(9,50)) +
        geom_smooth(method = "lm", formula = y ~ I(1/x^9), fullrange = TRUE) +
        geom_hline(yintercept = 100) +
        xlab("Number of Training Data Samples") +
        ylab("In-sample Accuracy [%]") +
        theme(text = element_text(size=12), axis.text.x = element_text(hjust=1, size=10),
        axis.text.y = element_text(hjust=1, size=10))
######################
multiplot(p1, p2, p3, p4, p5, p6, cols = 3)

```

---
## Slide 6 - Final Thoughts and Further Reading
Researchers have devised a much better model for predicting sample size compared to what is used in this example. Specifically, they use an inverse power law to predict the learning rate of a classifier.

* Data scientists might consider developing learning curves as a technique during product development for applications and devices from which little data is available.

* For more information please refer to the following references:

Figueroa et al.: Predicting sample size required for classification performance. BMC Medical Informatics and Decision Making 2012 12:8.

Beleites et al.: Sample size planning for classification models. Analytica Chimica Acta, 2013, 760 (Special Issue: Chemometrics in Analytical Chemistry 2012), 25-33

